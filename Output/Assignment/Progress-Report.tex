% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{subfig}
\usepackage{float}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Progress Report},
  pdfauthor={Yufei Liu},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Progress Report\thanks{Code and data are available at: \url{https://github.com/Florence-Liu/Stellar-Flare}}}
\author{Yufei Liu}
\date{2025-03-03}

\begin{document}
\maketitle

\section{Introduction}\label{introduction}

Stellar flares are sudden, intense increases in the brightness of stars, caused by magnetic reconnection events in their atmospheres. These energetic outbursts can provide valuable insights into stellar activity and characteristics of the stars. The Transiting Exoplanet Survey Satellite (TESS) from NASA provides high-precision light curve data that can be used to detect and analyze these flares.

In this study, we explore flare detection methods using TESS light curve data. Our approach focuses on anomaly detection methods, given the unique characteristics of flares as deviations from the typical brightness variations of a star. Specifically, we investigate time series models, DBSCAN, Gaussian Mixture Models (GMM), and Isolation Forest to detect flares based on residual and outlier identification, and density-based clustering. This study aims to assess the feasibility of unsupervised learning for stellar flare detection and evaluate the performance of different methods in identifying transient stellar events.

\section{Data}\label{data}

To explore different anomaly detection approaches, we use light curve data for star TIC 129646813 from TESS mission, which is available on the MAST website. The dataset consists of time-series observations of stellar flux, where each observation captures the brightness of a star over time. These observations are taken at regular intervals, typically every 2 minutes (short-cadence mode). We use the Pre-search Data Conditioned Simple Aperture Photometry (PDCSAP) flux to conduct our analysis since it is clearer and contains less noise due to detrending manipulations. Flares appear as transient increases in flux, often characterized by a rapid rise followed by a gradual decay. In this study, we preprocess the data to handle missing values and standardize the flux measurements before applying unsupervised learning methods to identify potential flare events as anomalous deviations from the expected stellar variability.

As shown in Figure \ref{fig:data-1}, there is a gap in time between 1338 and 1340. Since the data was collected over a regular time interval, such gap may due to instrumental errors. Also, there exists a few missing values in flux at the end of the time series. Since for some methods we are interested in, such as ARIMA and DBSCAN, the algorithm could not handle missing value naturally, thus we need to either remove or impute the missing values. As shown in Figure \ref{fig:data-2}, the yellow line represents the imputation result using ARIMA interpolation in \texttt{imputeTS} package in R. It fills both the gap in time and missing values in flux.

When imputing the missing values, we assume that the behaviour of the star's flux has the same pattern as it was before the gap and after the gap However, to validate the necessity of imputation, the following analysis will compare the model results for both imputed data and original data.

\begin{figure}[H]

{\centering \subfloat[original\label{fig:data-1}]{\includegraphics[width=0.5\linewidth]{Figure/ts_plot_129} }\subfloat[with imputation\label{fig:data-2}]{\includegraphics[width=0.5\linewidth]{Figure/ARIMA_ImputeTS_129} }

}

\caption{Light curve time series for TIC 129646813}\label{fig:data}
\end{figure}

\section{Methods}\label{methods}

\subsection{Time Series Analysis}\label{time-series-analysis}

To analyze variability in the light curve data and detect potential flare events, we model the time series using an autoregressive integrated moving average (ARIMA) approach, which captures temporal dependencies.

An ARIMA\((p,d,q)\) model consists of:

\begin{itemize}
    \item Autoregressive (AR) term ($p$): Captures relationships with past values.
    \item Integrated (I) term ($d$): Differencing ensures stationarity.
    \item Moving Average (MA) term ($q$): Models short-term fluctuations using past errors.
\end{itemize}

Mathematically, it is defined as:
\begin{equation}
    \Phi_p(B)(1 - B)^d Y_t = \Theta_q(B) \epsilon_t,
\end{equation}
where \(B\) is the backshift operator, \(\Phi_p(B)\) and \(\Theta_q(B)\) are polynomials of orders \(p\) and \(q\), respectively, and \(\epsilon_t\) is a white noise error term. This formulation enables ARIMA to model and forecast stellar brightness variations effectively.

We apply the \texttt{auto.arima} function to find the best fit model based on Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). After fitting the model, we examine the residuals and identify outliers in the residuals using a 3-sigma criterion, where points deviating more than three standard deviations from the mean residual are flagged as anomalies.

\subsection{Machine Learning Methods}\label{machine-learning-methods}

\subsubsection{Isolation Forest}\label{isolation-forest}

We then apply the Isolation Forest (IF), an unsupervised anomaly detection method based on recursive partitioning. The key idea is that anomalous points, being rare and different from the majority, are easier to isolate with fewer splits.

Given a dataset \(X = \{x_1, x_2, \dots, x_n\}\), the Isolation Forest algorithm operates as follows:

\begin{enumerate}
    \item Construct multiple isolation trees (iTrees) by recursively splitting random subsets of the data along randomly chosen features.
    \item Compute the anomaly score of each point $x$ based on its average isolation path length:
    \begin{equation}
        s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}
    \end{equation}
    where $E(h(x))$ is the expected path length and $c(n)$ is a normalization factor.
    \item Points with high anomaly scores are flagged as potential outliers.
\end{enumerate}

We use the \texttt{IsolationForest} function fro \texttt{sklearn} library in Python to fit the time series data and visualize the anomaly detection results. For IF, the algorithm could handle the missing values and irregular time space, so we don't need furthur manipulation on data.

\subsubsection{Gaussian Mixture Model}\label{gaussian-mixture-model}

Gaussian Mixture Model (GMM) is a probabilistic approach that represents the data as a mixture of multiple Gaussian components. We could also use it to detect anomaly, which can be seen as the low-likelihood points.

The GMM assumes the observed data \(\mathcal{X} = \{x_1, x_2, \dots, x_n\}\) follows a mixture of \(K\) Gaussian distributions:

\begin{equation}
    p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k),
\end{equation}

where \(\pi_k\) are the mixing weights, and \(\mathcal{N}(x \mid \mu_k, \Sigma_k)\) represents a Gaussian component with mean \(\mu_k\) and covariance \(\Sigma_k\). The parameters \((\pi_k, \mu_k, \Sigma_k)\) are estimated using the Expectation-Maximization (EM) algorithm.

We use the \texttt{GaussianMixture} function fro \texttt{sklearn} library in Python to fit the time series data and visualize the anomaly detection results. For GMM, the algorithm could not handle the missing values, so we need to handle the missing values before fitting.

\subsubsection{DBSCAN}\label{dbscan}

Another common method used for anomaly detection is Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a clustering algorithm that detects anomalies as points that do not belong to any dense cluster. It partitions data into clusters based on their distance to other points.

We use the \texttt{DBSCAN} function fro \texttt{sklearn} library in Python to fit the time series data and visualize the anomaly detection results. For DBSCAN, the algorithm could not handle the missing values as well as the irregular time space, so we also need to handle the missing values before fitting.

\section{Results}\label{results}

\subsection{Time Series Model}\label{time-series-model}

The best fit ARIMA model for the light curve data is ARIMA\((4,1,1)\), which indicates:

\begin{itemize}
    \item $p = 4$: Four lagged terms are included to account for autocorrelation.
    \item $d = 1$: Differencing is applied once to remove trends and ensure stationarity.
    \item $q = 1$: One lagged error term is included to model short-term fluctuations.
\end{itemize}

Figure \ref{fig:ts} shows the the model results for imputed data and Figure \ref{fig:ts2} shows the the model results for the original data. The red points are the detected outliers with the blue dashed line representing the upper and lower threshold of 3 standard deviations. For the residual plots, we could see that the are mostly normally distributed around 0 while heavy right tails may indicate the outliers.

Since the \texttt{auto.arima} function handle the irregular time space, and the final results seems to be very similar for the two datasets, considering extra bias introduced by the imputation, we could keep the original one and just remove observations with missing flux value at the end.

\begin{figure}[H]

{\centering \subfloat[result\label{fig:ts-1}]{\includegraphics[width=0.5\linewidth]{Figure/arima_129} }\subfloat[residual\label{fig:ts-2}]{\includegraphics[width=0.5\linewidth]{Figure/res_arima_129} }

}

\caption{ARIMA model results for imputed data}\label{fig:ts}
\end{figure}

\begin{figure}[H]

{\centering \subfloat[result\label{fig:ts2-1}]{\includegraphics[width=0.5\linewidth]{Figure/arima_129_without_imputation} }\subfloat[residual\label{fig:ts2-2}]{\includegraphics[width=0.5\linewidth]{Figure/res_arima_129_without_imputation} }

}

\caption{ARIMA model results for original data}\label{fig:ts2}
\end{figure}

\subsection{Machine Learning Model}\label{machine-learning-model}

Figure \ref{fig:ml} shows the the model results for imputed data and Figure \ref{fig:ml2} shows the the model results for the original data. Figure \ref{fig:ml-1} and Figure \ref{fig:ml2-1} show the result for Isolation Forest with outliers marked as -1. So we could see that points in blue are the resulted anomaly. It is noticed that the period we imputed actually has no anomaly detected, aligning with the assumption.

\begin{figure}[H]

{\centering \subfloat[IF\label{fig:ml-1}]{\includegraphics[width=0.5\linewidth]{Figure/IF_1_129} }\subfloat[GMM\label{fig:ml-2}]{\includegraphics[width=0.5\linewidth]{Figure/GMM_1_129} }\newline\subfloat[DBSCAN\label{fig:ml-3}]{\includegraphics[width=0.5\linewidth]{Figure/DBSCAN_1_129} }

}

\caption{Machine learning model results for imputed data}\label{fig:ml}
\end{figure}

Figure \ref{fig:ml-2} and Figure \ref{fig:ml2-2} shows the result for the GMM with anomaly marked as 1, so in this case, the red lines are the detected anomaly. For Figure \ref{fig:ml-3} and Figure \ref{fig:ml2-3}, the plots show detected results for DBSCAN where 1 indicates anomaly. The Isolation Forest and GMM give similar results, however, DBSCAN gives much fewer detected outliers.

To better validate if the imputation gives different results from the original dataset, we plot the density distribution of anomaly score in the IF model as shown in Figure \ref{fig:comp}. We could see that the red line shifts rightward, which indicates it is more likely to detect a point as an outlier in the imputed data. This also aligns with Figure \ref{fig:comp-2} that IF detects more outliers with imputed data. While other two methods shows similar results for the original data and imputed data, the IF algorithm actually gives a different result.

\begin{figure}[H]

{\centering \subfloat[IF\label{fig:ml2-1}]{\includegraphics[width=0.5\linewidth]{Figure/IF_1_129_without_imputation} }\subfloat[GMM\label{fig:ml2-2}]{\includegraphics[width=0.5\linewidth]{Figure/GMM_1_129_without_imputation} }\newline\subfloat[DBSCAN\label{fig:ml2-3}]{\includegraphics[width=0.5\linewidth]{Figure/DBSCAN_1_129_without_imputation} }

}

\caption{Machine learning model results for the original data}\label{fig:ml2}
\end{figure}

\begin{figure}[H]

{\centering \subfloat[Anomaly Score\label{fig:comp-1}]{\includegraphics[width=0.5\linewidth]{Figure/IF_comparison_129} }\subfloat[Total Number\label{fig:comp-2}]{\includegraphics[width=0.5\linewidth]{Figure/IF_comparison_129_num} }

}

\caption{Comparison for Isolation Forest on the original data and imputed data}\label{fig:comp}
\end{figure}

\section{Discussion}\label{discussion}

\subsection{Current Work}\label{current-work}

After comparing 4 model results for the original data and imputed data, we decide to use the original dataset since the model themselves could handle the small time gap. Also, since the missing time gap was due to instrumental errors, they were not missing by random, the imputation may introduce extra bias (as in the IF algorithm). As a result, for further model fitting, we will focus on the original dataset.

\subsection{Next Steps}\label{next-steps}

The model, especially the machine learning models, are just preliminary since I used the default parameter setting in the function. To better fit the models, I will then tune the hyperparameters in the model and fit on the original dataset. In this case, I would consider grid search to find the best combinations of parameter values. The key point here may be to find the key metrics for grid search since we only have unlabeled data.

Another thing is to validate the models since our goal is also to compare the model performance. Since the data is unlabelled, and we use unsupervised learning, I may consider perform the models on the simulated dataset and to see how it works. Also, since we used both machine learning methods and time series models, it may hard to find a general metric that applies to all of the models.

\end{document}
